{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taining data\n",
    "Data_train = pd.read_csv('mushroom_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "Data_test = pd.read_csv('mushroom_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training data features and labels\n",
    "X = Data_train.drop(columns='class')\n",
    "y = Data_train['class']\n",
    "\n",
    "# Define column transformer\n",
    "numerical_features = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "categorical_features = ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed',\n",
    "                        'gill-attachment', 'gill-spacing', 'gill-color', 'stem-color', 'has-ring',\n",
    "                        'ring-type', 'habitat', 'season']\n",
    "\n",
    "# Feature Engineering\n",
    "X_new = X.copy()\n",
    "\n",
    "# Iterate over the selected categorical features\n",
    "for feature in categorical_features:\n",
    "    stats_df = X.groupby(feature).agg(['mean', 'min', 'max', 'median'])[numerical_features]\n",
    "    stats_df.columns = [f\"{feature}_{num_feat}_{stat}\" for num_feat in numerical_features for stat in ['mean', 'min', 'max', 'median']]\n",
    "    stats_df.reset_index(inplace=True)\n",
    "    X_new = X_new.merge(stats_df, on=feature, how='left')\n",
    "\n",
    "# Define column transformer\n",
    "column_transformer = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "D = 14  # Change this value to select the top D features\n",
    "feature_selector = SelectKBest(score_func=f_classif, k=D)\n",
    "\n",
    "# Create the pipeline\n",
    "transformer = Pipeline(steps=[('transformer', column_transformer),\n",
    "                              ('selector', feature_selector)])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_data = le.fit_transform(y)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_data = transformer.fit_transform(X_new, y_data)\n",
    "\n",
    "# Split data into training data and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate test data features and labels\n",
    "Xtest = Data_test.drop(columns='class')\n",
    "ytest = Data_test['class']\n",
    "\n",
    "# Feature Engineering for test data\n",
    "X_test_new = Xtest.copy()\n",
    "\n",
    "# Iterate over the selected categorical features\n",
    "for feature in categorical_features:\n",
    "    stats_df = X.groupby(feature).agg(['mean', 'min', 'max', 'median'])[numerical_features]\n",
    "    stats_df.columns = [f\"{feature}_{num_feat}_{stat}\" for num_feat in numerical_features for stat in ['mean', 'min', 'max', 'median']]\n",
    "    stats_df.reset_index(inplace=True)\n",
    "    X_test_new = X_test_new.merge(stats_df, on=feature, how='left')\n",
    "\n",
    "# Transform test data using the same pipeline\n",
    "X_test = transformer.transform(X_test_new)\n",
    "\n",
    "# Encode the test labels\n",
    "y_test = le.transform(ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trivial system\n",
    "unique_labels, counts = np.unique(y_data, return_counts=True)\n",
    "\n",
    "probability = counts / counts.sum()\n",
    "\n",
    "y_pred = np.random.choice(unique_labels, size=len(y_test), p=probability)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Trivial system accuracy:\", accuracy)\n",
    "print(\"Trivial system F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Means Classifier\n",
    "def classifier(X, y):\n",
    "    means = {}\n",
    "    for label in np.unique(y):\n",
    "        class_data = X[y == label]\n",
    "        class_mean = np.mean(class_data, axis=0)\n",
    "        means[label] = class_mean\n",
    "    return means\n",
    "\n",
    "def predict_nearest_mean(X, means):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        min = float('inf')\n",
    "        predicted_class = None\n",
    "        for label, class_mean in means.items():\n",
    "            distance = np.linalg.norm(x - class_mean)\n",
    "            if distance < min:\n",
    "                min = distance\n",
    "                predicted_class = label\n",
    "        predictions.append(predicted_class)\n",
    "    return np.array(predictions)\n",
    "\n",
    "print(\"Nearest Mean Classifier\")\n",
    "# Train the Nearest Mean Classifier\n",
    "means = classifier(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred_val = predict_nearest_mean(X_val, means)\n",
    "\n",
    "# Calculate the accuracy and F1-score for the validation set\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_val)\n",
    "print(\"Validation F1-score:\", f1_val)\n",
    "print()\n",
    "\n",
    "# Train Nearest Mean Classifier on full training set\n",
    "class_means_full = classifier(X_data, y_data)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_nc = predict_nearest_mean(X_test, class_means_full)\n",
    "\n",
    "# Calculate the accuracy and F1-score for the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred_nc)\n",
    "f1_test = f1_score(y_test, y_pred_nc, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy_test)\n",
    "print(\"Test F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with linear kernel\n",
    "print(\"SVM with linear kernel\")\n",
    "svm_classifier = SVC(kernel='linear', C=0.1)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy = svm_classifier.score(X_train, y_train)\n",
    "\n",
    "y_pred_val = svm_classifier.predict(X_val)\n",
    "accuracy_val = svm_classifier.score(X_val, y_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Train accuracy:\", train_accuracy)\n",
    "print(\"Validation accuracy:\", accuracy_val)\n",
    "print(\"Validation F1-score:\", f1_val)\n",
    "\n",
    "svm_classifier.fit(X_data, y_data)\n",
    "\n",
    "y_pred_test = svm_classifier.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='binary', pos_label=1)\n",
    "\n",
    "test_accuracy = svm_classifier.score(X_test, y_test)\n",
    "\n",
    "print()\n",
    "print(\"Test accuracy:\", accuracy_test)\n",
    "print(\"Test F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with rbf kernel\n",
    "svm_classifier = SVC(kernel='rbf', C=10, gamma=1)\n",
    "\n",
    "# Train SVM model on the entire training set\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy and F1-score on validation set\n",
    "train_accuracy = svm_classifier.score(X_train, y_train)\n",
    "accuracy_val = svm_classifier.score(X_val, y_val)\n",
    "y_pred_val = svm_classifier.predict(X_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Train accuracy:\", train_accuracy)\n",
    "print(\"Validation accuracy:\", accuracy_val)\n",
    "print(\"Validation F1-score:\", f1_val)\n",
    "print()\n",
    "\n",
    "# Evaluate accuracy and F1-score on test set\n",
    "y_pred_test = svm_classifier.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='binary', pos_label=1)\n",
    "test_accuracy = svm_classifier.score(X_test, y_test)\n",
    "    \n",
    "print(\"Test accuracy:\", accuracy_test)\n",
    "print(\"Test F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neighbors\n",
    "n_neighbors = 5\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict for validation data\n",
    "y_pred_val = knn.predict(X_val)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Validation Accuracy: {accuracy_val}\")\n",
    "print(\"Validation F1-score:\", f1_val)\n",
    "print()\n",
    "\n",
    "# Predict for test data\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the test data\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Test F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-class Perceptron\n",
    "def perceptron(X, y, n_epochs, learning_rate, batch_size, l2_reg):\n",
    "    n_samples, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        \n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_X = X[i:i + batch_size]\n",
    "            batch_y = y[i:i + batch_size]\n",
    "            \n",
    "            y_pred = np.dot(batch_X, weights) + bias\n",
    "            y_pred = np.where(y_pred > 0, 1, -1)\n",
    "            errors = batch_y - y_pred\n",
    "            weights += learning_rate * (np.dot(batch_X.T, errors) - l2_reg * weights)\n",
    "            bias += learning_rate * np.sum(errors)\n",
    "    \n",
    "    return weights, bias\n",
    "\n",
    "def predict(X, weights, bias):\n",
    "    y_pred = np.dot(X, weights) + bias\n",
    "    return np.where(y_pred > 0, 1, 0)\n",
    "\n",
    "# Train the perceptron\n",
    "n_epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "l2_reg = 0.0001\n",
    "weights, bias = perceptron(X_train.toarray(), y_train * 2 - 1, n_epochs, learning_rate, batch_size, l2_reg)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred_train = predict(X_train.toarray(), weights, bias)\n",
    "\n",
    "# Calculate the accuracy and F1-score for the train set\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "f1_train = f1_score(y_train, y_pred_train, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Train accuracy:\", accuracy_train)\n",
    "print(\"Train F1-score:\", f1_train)\n",
    "print()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_val = predict(X_val.toarray(), weights, bias)\n",
    "\n",
    "# Calculate the accuracy and F1-score for validation set\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "f1_val = f1_score(y_val, y_pred_val, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Validation accuracy:\", accuracy_val)\n",
    "print(\"Validation F1-score:\", f1_val)\n",
    "print()\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_test = predict(X_test.toarray(), weights, bias)\n",
    "\n",
    "# Calculate the accuracy and F1-score test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy_test)\n",
    "print(\"Test F1-score:\", f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "# The code is sampled from the EE559 Github page \"https://github.com/keithchugg/ee559_spring2023/blob/main/lecture/fmnist_mlp_torch.ipynb\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Parameters\n",
    "n_hidden = 128\n",
    "n_epochs = 40\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.001 \n",
    "n_features = X_data.shape[1]\n",
    "n_classes = 2\n",
    "n_splits = 5\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_data.toarray(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a DataLoader for test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define KFold cross-validator\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# Training with cross-validation\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(X_tensor, y_tensor)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    X_train, y_train = X_tensor[train_indices], y_tensor[train_indices]\n",
    "    X_val, y_val = X_tensor[val_indices], y_tensor[val_indices]\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = MLP(n_features, n_classes, n_hidden).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    f1_list = []\n",
    "    val_accuracy_list_fold = []\n",
    "    f1_list_fold = []\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss_list.append(running_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='binary', pos_label=1)\n",
    "        val_accuracy_list.append(accuracy)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    val_accuracy_list_fold.append(accuracy)\n",
    "    f1_list_fold.append(f1)\n",
    "        \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "print(f\"Average Validation Accuracy: {np.mean(val_accuracy_list_fold):.4f}\")\n",
    "print(f\"Average Validation F1-score: {np.mean(f1_list_fold):.4f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, n_epochs+1), train_loss_list, label=\"Train Loss\")\n",
    "plt.plot(range(1, n_epochs+1), val_accuracy_list, label=\"Validation Accuracy\")\n",
    "plt.plot(range(1, n_epochs+1), f1_list, label=\"Validation F1 Score\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss | Accuracy | F1 Score\")\n",
    "plt.title(\"Loss | Accuracy | F1 Score vs Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "        test_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and F1 score on test data\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='binary', pos_label=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test F1-score: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee559_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
